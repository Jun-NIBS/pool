\subsection{Existing Approaches} \label{sec:existing approaches}

We will contrast POOL with the existing way in which statistically-based predictions are used in the discovery of peptides, small molecules, and in other molecular discovery applications.
We focus on situations in which activity $y(e)$ is binary. In existing approaches, one would first collect some training data, in the form of peptide/activity pairs $(e,y(e))$, and train a machine learning classifier to make predictions $\hat{y}(e)$ of $y(e)$.  
The classifier might be a naive Bayes classifier like the one that we use in Section~\ref{sec:stat model}, or some other classifier, e.g., a support vector machine \citep{cortes1995support}, a random forest \citep{liaw2002classification}, or a neural network \citep{haykin2004comprehensive}.
Depending on the classifier, the prediction may be binary, $\hat{y}(e) \in \{0,1\}$, or may be a real-valued score, $\hat{y}(e) \in [0,1]$ indicating a level of confidence, with values close to $0$ or $1$ indicating a high level of confidence, and values close to $1/2$ indicating a high level of uncertainty.
Bayesian approaches even predict a joint distribution over $\left(y(e): e \in E \right)$, and the distribution will be updated when new label $y(e)$ is revealed upon testing a new peptide $e \in E$, reflecting addition of new information.

For applications in which our goal is \eqref{eq:general problem}, and in each round of experiment we are allowed to test $k$ peptides, there are two main existing ways to propose the $k$ peptides for testing:
\begin{itemize}
\item 
Choose a minimum required score $a$, and rank those peptides with $\hat{y}(e) \geq a$ according to their fitness.
% Rank peptides with $\hat{y}(e) = 1$  by $f(e)$.
% If $\hat{y}(e)$ is score-based, then one may rank those peptides whose score exceeds some minimum threshold.
\item
Choose a minimum required fitness $b$, and rank those peptides with fitness exceeding $b$, i.e., those with $f(e) \geq b$, according to their score $\hat{y}(e)$. Test the peptides in this order.
% \item 
% Rank all peptides with $\hat{y}(e)\geq b'$, where $b'$ is some threshold, according to $f(e)$.
% [This method is really equivalent to the first method, because we can take a real-valued prediction, and threshold it, to obtain a binary-valued prediction.]
% \item
% We could take both $f(e)$ and real-valued score into account, formulate it as a multi-objective optimization problem to rank by both quantity.
\end{itemize}

The first method works well when a high fraction of highly-scored peptides are active, in which case one may focus on finding an active peptide with fitness as large as possible. In practice, finding even a single active peptide with reasonably large fitness is challenging, and so the second method 
builds in robustness by supposing only that score is correlated with activity, and that testing peptides in order of their score is likely to reveal one with activity in a small number of measurements.
In exchange for this robustness, it does not focus on finding the largest fitness $f(e)$, instead attempting to find one with fitness that is ``good enough''.
% focuses only on finding an active peptide whose fitness is ``good enough''.
The second method can also be used when there is no fitness, and the only measure of quality is activity.

We call both of these approaches ``predict-then-optimize'' approaches, because they make a single batch of predictions, which they then imagine stays fixed during the course of measurement.
They then optimize either the score or the fitness.

We will compare in detail against the second of these predict-then-optimize methods, which is much more common in the literature~\cite{agarwal2010ranking, ballester2010machine, smith2010sirt3}.   

When the  the classifier in use produces a joint probability distribution over $\left(y(e): e \in E\right)$ (the Bayesian Naive Bayes classifier in section~\ref{sec:stat model} is one example), the second and more common predict-then-optimize approach 
recommends testing
\begin{equation*}
  \underset{e \in E, f(e) \ge b}{\arg\max} \, P(y(e)=1)
  \label{}
\end{equation*}
Later, we will refer to this predict-then-optimize approach alone.

While the predict-then-optimize approach includes some robustness to prediction error, it does not consider how future activity measurements will change the score, and how in turn this should affect the order in which peptides are tested.  We argue below  in Section~\ref{sec:contrast with predict-then-optimize} that this makes the approach unnecessarily brittle to inaccuracies in predictions, and causes the set of peptides tested to lack diversity.  
Indeed, if the first peptide tested by the predict-then-optimize approach is not a hit, then the second peptide is typically quite similar, and is likely to also not be a hit.

By considering the relationships between peptides' scores, and how measuring one peptide's activity will change the prediction of another's activity, peptide optimization with optimal learning (POOL) performs a more complete accounting of the value of testing a particular set of peptides, which causes it to recommend a more diverse set of peptides. 
We argue below that this leads POOL to provide more robust performance, that finds high-fitness active peptides in fewer experiments than the predict-then-optimize approach.

% The predict-then-optimize approach assumes that the predictions $\hat{y}(e)$ are correct, and then solves, or approximately solves, the optimization problem 
% \begin{equation*}
%   \underset{e \in E, \hat{y}(e) = 1}{\arg\max} \, f(e).
%   \label{}
% \end{equation*}

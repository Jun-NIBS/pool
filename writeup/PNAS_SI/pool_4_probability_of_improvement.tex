\subsection{Probability of Improvement} \label{sec:prob improvement} 
Many prediction methods, including all Bayesian prediction methods, provide a joint probability distribution over $y(e): e \in E$, which encodes prediction uncertainty. In our optimal learning approach, we use this joint probability distribution to define an auxiliary function, called probability of improvement, which quantifies the probability that a set of peptides $S$, if tested, will reveal an active peptide whose fitness $f(e)$ improves on some benchmark value $b$, which is typically the best $f(e)$ of active peptides observed in the past. We define this probability-of-improvement, $\PI(S)$, to be
\begin{equation}
  \PI(S) = \Prob \left( \max_{e \in S, y(e)=1} f(e) > b \right).
  \label{}
\end{equation}
To ensure the event that none of the peptides we test are active does not contribute to probability-of-improvement, we define $\max$ over an empty set is $-\infty$. We find $S$ by solving
\begin{equation}
  \underset{S \subseteq E, |S| \leq k}{\arg\max} \PI(S),
  \label{eq:opt PI}
\end{equation}
where $k$ is the number of peptides that can be tested simultaneously in a single round of experiment. 

After we find the set $S$ and tested the peptides in it, we use the results along with previously tested peptides as training data, and find a new set $S$ for the next round of experiment. We repeat this process until we find enough favored active peptides or resource is exhausted. We demonstrate in an application of finding reversible chemoenzymatic labeling peptides that this approach performs better than existing approaches. This use of the probability of improvement builds on previous work in engineering for global optimization of time-consuming computer codes \citep{kushner1964new, jones1998efficient}, and also on value-of-information and optimal learning methods in that same application area \citep{villemonteix2009informational, frazier2009knowledge}.


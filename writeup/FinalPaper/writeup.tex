\documentclass[12pt]{article}
\usepackage{amsmath,amssymb,mathtools}
\usepackage{bbm}
\newcommand{\Prob}{\mathbb{P}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\EI}{\mathrm{EI}}
\newcommand{\Dir}{\mathrm{Dirichlet}}
\newcommand{\PI}{\text{P}^*}
\newcommand{\mb}{\mathbf}

\begin{document}
\section{Introduction}

\section{Problem Statement and Application}
We first describe the application that motivates our research, and then we provide mathematical formalism to address a more general problem. In the last sub-section we derive our method in solving this problem.

\subsection{Motivating application}
We have two enzymes (Sfp from {\it Bacillus subtilis}, and PaAcpH from {\it Pseudomonas aeruginosa}), and a collection of peptides that can potentially act as a substrate for one or both of these enzymes.  Our goal is to find a peptide that acts as a substrate for both of these enzymes, and is as short as possible.

To support this goal, we can do lab experiments, in which we synthesize a peptide and test, for each enzyme, whether it is a substrate or not.  We need to find a policy that suggests which peptide to synthesize and test next, so as to reach our goal with as few experiments as possible.

Experiments have parallel setup, thus can be done with a batch of peptides at a time, and so the algorithm suggests a batch of peptides at a time, waiting for the results from the experiment before suggesting the next batch of peptides.
A large collection of peptides would be considered by the algorithm for potential synthesis and testing, e.g., all peptides with length less than a given threshold.  That is, we would consider more peptides than just those that are sub-peptides of peptides from the literature known to be substrates for one enzyme.

\subsection{General Problem Statement}
We now formalize and generalize our problem as an active learning problem, which includes but is not limited to our motivating application.

Let $E$ be a generic search space of exemplars.  In our motivating application, $E$ is the space of peptides.
Each element $x \in E$ has an unknown binary label $y(x)=\{0,1\}$.  A known deterministic function $f(x)$ measures the cost or disutility associated with $x$. Our goal is to perform experiments so as to find $x$ such that it has positive label and its cost function $f(x)$ is minimum.

To obtain labels of exemplars, we can do a batch of experiments, which evaluate a subset $S \subseteq E$ and obtain labels at each time. We measure quality of $S$ by
\begin{equation} \label{eq:fS}
f^*(S)= \begin{dcases}
 \underset{x \in S:y(x)=1}{\min} f(x), & \text{if \,} \{x \in S:y(x)=1\} \neq \emptyset, \\
 \infty,  & \text{if \,} \{x \in S:y(x)=1\} = \emptyset.
 \end{dcases}
\end{equation}

%Let $E$ be any set. For each element $x\in E$, we define a function $f(x)$ that measures the quality of $x$. Larger or smaller values of $f(x)$ may be favored depending on different problem settings. From now on, we assume, without losing generality that smaller values of $f(x)$ are preferred, and for any subset $S\subseteq E$, we measure the quality of $S$ as:
%
%\begin{equation*}
%f^*(S) = \min_{x\in S:\mathbf{h}(x)=0}f(x)
%\end{equation*}
%
%where $\mathbf{h}(x)=(h_1(x),\cdots,h_m(x))$ is a set of constraints that define a subset of "effective elements". We wish to find $S\subseteq E$ with $f^*(S)$ as small as possible, while $S$ it self must satisfy some constraints. A typical constraint is the cardinality of $S$, we usually prefer smaller sets. Other constraints can be applied in different problems.

Let $b$ be a target value and we wish to find $S\subseteq E$ such that $f^*(S)$ is, in some sense, better than $b$. Specifically, we consider the following two measures:
\begin{equation} \label{eq:twomeasure}
\begin{aligned}
&\text{Probability of Improvement: }&\PI(S) = \mathbb{P}(f^*(S) < b)\\
&\text{Expected Improvement: }&\EI(S) = \E [(b-f^*(S))^+]
\end{aligned}
\end{equation}
We wish to find $S$ that maximize one of these two measures. Let $g(S)$ be either $\PI(S)$ or $\EI(S)$ and let the cardinality of $S$ be the only constraint on $S$. Our goal is then:

\begin{equation} \label{eq:opt}
\max_{S\subseteq E:|S|<k}g(S)
\end{equation}

\section{Solution Method}
%If $g(S)$ is $\EI(S)$, from equation \eqref{eq:twomeasure} \eqref{eq:opt}, our goal becomes
%\begin{equation} \label{eq:maxEI}
%\underset{S \subseteq E:|S| \leq k}{\max} \E \left[ (b-f^*(S))^+ \right]
%\end{equation}
%We first prove that the objective function is submodular, and then describe our greedy approach to solve \eqref{eq:maxEI}, finally we show that we have guarantee for our greedy policy compared with optimal policy.
We solve \eqref{eq:opt} by greedy algorithm, that is, starting with empty set $S=\emptyset$, find element $e = \mathrm{arg}\max_e g(S \cup \{e\})-g(S)$ to include in $S$ iteratively until $|S|=k$ for some chosen k. We can show the greedy solution has lower bound, which is a factor $(1-1/ e)$ of the optimal objective value.
\subsection{Lower bound of greedy algorithm}
\subsection{Algorithm}
Suppose we have chosen $S=\{x_1, x_2, \ldots, x_n\}$ as a batch of points we are going to evaluate next, and if we want to incorporate one more point $e$, which is distinct from $x_1, x_2, \ldots, x_n$, such that the expected improvement increases most, we use the following criterion to find $e$:
\begin{equation} \label{eq:finde}
\underset{e \in E \backslash S}{\mathrm{arg}\max} \, \E \left[ (b-f^*(S \cup \{e\}))^+ \right]
\end{equation}
From \eqref{eq:fS} we write expected improvement part in \eqref{eq:opt} as
\begin{align*}
\E \left[ (b-f^*(S))^+ \right] &= \E [b-\underset{x \in S \cup \{e\}:y(x)=1}{\min} f(x)] \\
                  &=
                  \begin{dcases}
                    \E [b- f^*(S)] & \text{if $y(e)=0$ ,} \\
                    \E [b- \min \{f(e), f^*(S)\}]       & \text{if $y(e)=1$ ,}
                    \end{dcases} \\
                  &= \E [b-f^*(S) + \mathbbm{1}_{\{y(e)=1, f(e) < f^*(S)\}} [f^*(S)-f(e)] ]
\end{align*}
After some algebra we can write \eqref{eq:finde} as
\begin{align*}
&\underset{e \in E \backslash S}{\mathrm{arg}\max} \E [ \mathbbm{1}_{\{y(e)=1, f(e) < f^*(S)\}} [f^*(S)-f(e)] ] \\
%&=\underset{e \in E \backslash S}{\mathrm{arg}\max} \E [ \E [\mathbbm{1}_{\{y(e)=1, f(e) < f^*(S)\}} [f^*(S)-f(e)] | f^*(S)]\\
%&= \underset{e \in E \backslash S}{\mathrm{arg}\max} \E [ \mathbbm{1}_{\{f(e)<f^*(S)\}} \mathbb{P} (y(e)=1 | f^*(S)) [f^*(S)-f(e)]] \\
&= \underset{e \in E \backslash S}{\mathrm{arg}\max} \sum_{i=1}^{|S|} \mathbb{P} (y(e)=1, y(x_i)=1, y(x_j)=0, \forall j <i) [f(x_i)-f(e)]^+ \\
&+ \mathbb{P} (y(e)=1, y(x_j)=0, \forall j) [b-f(e)]^+
\end{align*}
where $f(x_i)<=f(x_j)$ for $\forall i<j, x_i,x_j \in S$.
Since
\begin{align*}
&\mathbb{P} (y(e)=1, y(x_i)=1, y(x_j)=0, \forall j <i)\\
&= \mathbb{P}(y(x_1)=0) \mathbb{P}(y(x_2)=0|y(x_1)=0) \ldots \mathbb{P}(y(e)=1|\mathcal{F}(x_1,x_2,\ldots,x_i)\\
&\propto \mathbb{P}(y(e)=1|\mathcal{F}(x_1,x_2,\ldots,x_i)
\end{align*}



\section{Application}
\subsection{Statistical Method}

We use Naive Bayes as the classification method, which, despite the name, has performed quite well in many cases. Let $X=(X_1,\ldots,X_n)$ be an instance with $n$ features and $Y$ be its label. By Bayes's Rule, we have:

\begin{equation*}
\Prob(Y=y|X=x)=\frac{\Prob(X=x|Y=y)\Prob(Y=y)}{\Prob(X=x)}=\frac{\Prob(X=x|Y=y)\Prob(Y=y)}{\sum_{y'}\Prob(X=x|Y=y')\Prob(Y=y')}
\end{equation*}

The Naive Bayes classifier assumes that the presence or absence of a particular feature is unrelated to the presence or absence of any other feature, given the class variable, i.e.

\begin{equation*}
\Prob(Y=y|X=x) = \frac{\prod_{i=1}^n\Prob(X_i=x_i|Y=y)\Prob(Y=y)}{\sum_{y'}\prod_{i=1}^n\Prob(X_i=x_i|Y=y)\Prob(Y=y)}
\end{equation*}

In our motivation application, we have a set of peptides, each with length less than or equal to $L$. Each peptide is a sequence of amino acids. We use a reduced alphabet for amino-acids, i.e., we group them into $K$ groups. For each peptide, let $A_i$ be the amino acid on position $i$, and let $X_i$ be the class of this amino acid. For a specific enzyme, let $Y(x)=1$ if peptide $x$ is a substrate for that enzyme and 0 if not.

We let $\theta_{y,i}(j)=\Prob(X_i=j|Y(X)=y)$, for each $i=1,\ldots,L$, $j=1,\ldots,K$ and $y\in\{0,1\}$. We further assume some known prior distribution $\Prob(Y(x)=y)$, $y\in\{0,1\}$. Let $\theta$ be the full set of parameters $\theta_{y,i}(j)$, for $i=1,\ldots,L$, $j=1,\ldots,K$ and $y\in\{0,1\}$. Then, given an unlabeled peptide, we can calculate its probability being a substrate as:

\begin{equation*} 
  \Prob\left(Y(x) = 1 | \theta\right) =
  \frac{\Prob(Y(x)=1) \prod_{i} \theta_{1,i}(x_i)}{
  \left[ \Prob(Y(x)=1) \prod_{i} \theta_{1,i}(x_i)\right] +
  \left[ \Prob(Y(x)=0) \prod_{i} \theta_{0,i}(x_i)\right]}
\end{equation*}

We estimate the parameters $\theta_{y,i}(j)$ using Bayesian inference. We assume for each $i=1,\ldots,L$, $y\in\{0,1\}$, the vector $\theta_{y,i}\sim\Dir(\alpha_{y,i}(1),\ldots,\alpha_{y,i}(K))$. A good initial choice for the parameter vector $\alpha_{y,i} = (\alpha_{y,i}(1),\ldots,\alpha_{y,i}(6))$ can be choosing $\alpha_{y,i}(j)$ to be constant across $j$, and $y$, and to only depend upon $i$. Since amino acids further from the serine are less likely to have a strong influence on its activity, we choose this value to be $1$ in the positions next to the serine and to increase as $i$ moves further.

We further assume two hyper parameters $\gamma_0$ and $\gamma_1$ that characterize the distribution for $y=0$ and $y=1$ respectively. Then, with the prior distribution and hyper parameters, our posterior distribution is also Dirichlet. In particular, it is 
$\Dir( \alpha_{y,i}(1) + \gamma_yN_{y,i}(1), \ldots, \alpha_{y,i}(K) + \gamma_yN_{y,i}(K))$,
where $N_{y,i}(j)$ counts how many peptides $x$ in the training data with $Y(x)=y$ had $x_i=j$.  That is, it counts how many peptides had amino acid $i$ in class $j$.

Since our training data is expensive and highly skewed, we use the leave-one-out cross validation procedure to choose the optimal hyper parameters. For each setting of the hyper parameters, we obtain an receiver operating characteristic(ROC) curve using the result of the leave-one out procedure and choose the setting with highest AUC(area under curve).

[Put two ROC curves here, one for leave-one-out and one for using the 1st data set as training and 2nd data set as test, to be continued ...]


\end{document}

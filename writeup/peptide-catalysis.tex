\documentclass[12pt]{article}
\usepackage{amsmath,amssymb,mathtools}
\usepackage{setspace}
\usepackage[margin=1in]{geometry}
\usepackage{algorithmic}
\newcommand{\zap}[1]{ }
\newcommand{\R}{\mathbb{R}}
\newcommand{\data}{\text{training data}}
%\doublespacing

\title{An Active Learning Approach to Finding Minimally-sized Peptide Substrates}
\date{\today}
\author{(initial draft by Peter Frazier)} % authors TBD. Rough draft written by Peter Frazier
\author{
Michael Burkhart\thanks{Department of Chemistry and Biochemistry, UC San Diego} \and
Peter I. Frazier\thanks{School of Operations Research \& Information Engineering, Cornell University} \and
Nathan Gianneschi\footnotemark[1] \and
Michael K. Gilson\thanks{Skaggs School of Pharmacy and Pharmaceutical Sciences, UC San Diego}
}
\date{(author order currently alphabetical, other authors to be added as we go)\\ \today}


\begin{document}
\maketitle

\section{Problem Description}

We have two enzymes (Sfp from {\it Bacillus subtilis}, and PaAcpH from {\it Pseudomonas aeruginosa}, and a collection of peptides that can potentially act as a substrate for one or both of these enzymes.  Our goal is to find a peptide that acts as a substrate for both of these enzymes, and is as short as possible.

To support this goal, we can do lab experiments, in which we synthesize a peptide and test, for each enzyme, whether it is a substrate.

We also have an initial dataset gathered from the literature, % PF: add cite
which contains both peptides from nature, and peptides discovered using phage display.
Each peptide in this dataset is labeled as to whether or not it is a substrate for enzyme 1 (Sfp), and similarly for enzyme 2 (PaAcpH).
All peptides in the dataset are substrates for enzyme 1, and 8 out of 15 of them are substrates for enzyme 2.

We are designing an algorithm that suggests which peptide to synthesize and test next, so as to reach our goal with as few experiments as possible.

Experiments are done one peptide at a time, and so the algorithm suggests only one peptide at a time, waiting for the results from the experiment before suggesting the next peptide.
A large collection of peptides would be considered by the algorithm for potential synthesis and testing, e.g., all peptides with length less than a given threshold.  That is, we would consider more peptides than just those that are sub-peptides of peptides from the literature known to be substrates for one enzyme.

Here is a high level view of how work would flow:

\begin{enumerate}
\item Using the data from the literature, we train a classifier, whose output is not just an estimate of whether or not the peptide will be a substrate for each enzyme, but also the probability of each possibility.
  % (a substrate for both, a substrate for neither, a substrate for enzyme 1 but not enzyme 2, and, if it is possible, a substrate for enzyme 2 but not enzyme 1).
  This document proposes using Naive Bayes as our classification method, but other classification methods are also possible. (Section~\ref{sec:classifier})
\item While a short-enough peptide acting as a substrate for both enzymes has not yet been found, and we have the budget for additional experiments, we would do the following:
  \begin{enumerate}
    \item Based on the most recent Bayesian prior/posterior distribution, use a mathematical method described below to suggest which peptide to synthesize and test next. (Section~\ref{sec:design})
    \item Synthesize and test this peptide in the lab.
    \item Update the classifier to incorporate the most recent experimental results. (Section~\ref{sec:classifier})
  \end{enumerate}
\end{enumerate}

\section{Classification Method}\label{sec:NB}
\label{sec:classifier}

As an initial method, we propose using Naive Bayes, which actually performs quite well in many applications, despite the name.
This is a standard method, but we briefly review it in the context of this problem, and to introduce notation that is used later.

We represent a peptide by its sequence of amino acids, and refer to the index of an amino acid based on its position relative to the serine.  The serine itself is at position 0;
the amino acid immediately next to the serine, toward the N-terminal, is at position $-1$; one amino acid closer to the N-terminal is $-2$, etc.  Similarly, the amino acid next to the serine, toward the C-terminal, is at position $1$, then next to that is position $2$, etc.
We will consider positions from $-20$ up to $20$.

We additionally propose using a reduced amino acid alphabet, and have begun work using the Dayhoff classes described in \cite{Susko2004}.
There are 6 Dayhoff classes.  For a given peptide, we let $A_i$ be the amino acid at position $i$, and we let $X_i$ be the Dayhoff class of this amino acid.
We refer to a full peptide as $X$ or $x$ (we use a capital letter if we are thinking of the peptide as being drawn at random)\footnote{Note that this is potentially ambiguous, since two amino acids with the same sequence of Dayhoff classes would be referred to with the same value of $X$.  We may need to modify this notation, but we ignore this ambiguity for now.}.
For each enzyme $k=1,2$, let $Y(x,k)$ be $1$ if peptide $x$ is a substrate for enzyme $k$, and $0$ if not.\footnote{My understanding is that if $Y(x,1)=0$, then it is not possible to test whether $Y(x,2)$ is $0$ or $1$.  In this case, we assume that $Y(x,2)=0$.  Although this implies that $Y(x,1)$ is not independent of $Y(x,2)$, we will nevertheless assume it is independent, at least in the first version of this approach, for simplicity.}

The Naive Bayes classifier supposes that, if we fix one of the enzymes $k=1,2$,
and we draw a peptide $X$ at random from the set of peptides for which $Y(X,k)=1$, i.e., from the set of peptides that act as a substrate for enzyme $k$, then
for each $i\ne 0$,
\begin{equation} \label{eq:NB1}
  P\left(X_i = j | Y(X,k)=1, \theta^{(k)}\right) = \theta^{(k)}_{1,i}(j)
\end{equation}
where $j\in\{1,\ldots,6\}$ represents one of the six Dayhoff classes,
and the values $\theta^{(k)}_{1,i}(j)$, with $j=1,\ldots,6$ and $i=-20,\ldots,-1,1,\ldots,20$ are parameters to be estimated.\footnote{Since all peptides considered will have a serene, its presence does not allow us to differentiate substrates from non-substrates, so we simply skip $i=0$.}
Moreover, we assume that, conditioned on $Y(X,k)=1$, $\theta^{(k)}$, $X_i$ is independent across $i$.
This assumption is where the ``naive'' comes from in Naive Bayes.

We use a similar model for peptides that are not a substrate for enzyme $k$.
That is,
if we draw a peptide $X$ at random from the set of peptides for which $Y(X,k)=0$, then for $i\ne 0$,
\begin{equation} \label{eq:NB0}
  P\left(X_i = j | Y(X,k)=0, \theta^{(k)}\right) = \theta^{(k)}_{0,i}(j)
\end{equation}
for $j\in\{1,\ldots,6\}$.  Here as before $\theta^{(k)}_{0,i}(j)$, are parameters to be estimated.

We additionally assume some known prior distribution $P(Y(x,k)=1)$ for each $k=1,2$.
Initially, we choose $P(Y(x,k)=1) = P(Y(x,k)=0) = 1/2$.

Let $\theta^{(k)}$ represent the full set of parameters, $\theta^{(k)}_{y,i}(j)$, for $y=0,1$, $j=1,\ldots,6$ and $i=-20,\ldots,-1,1,\ldots,20$.
If we know $\theta^{(k)}$, then we can calculate the probability that
$Y(x,k)=1$, i.e., the probability that peptide $x$ is a substrate for enzyme
$k$, using Bayes rule.  This quantity is
\begin{equation} \label{eq:prob_Y_orig}
  P\left(Y(x,k) = 1 | \theta^{(k)}\right) =
  \frac{P(Y(x,k)=1) \prod_{i} \theta^{(k)}_{1,i}(x_i)}{
  \left[ P(Y(x,k)=1) \prod_{i} \theta^{(k)}_{0,i}(x_i)\right] +
  \left[ P(Y(x,k)=0) \prod_{i} \theta^{(k)}_{1,i}(x_i)\right]}
\end{equation}
where the product over $i$ is taken over all amino acids in the peptide, except for the serine at position $0$,
and $P(Y(x,k)=1)$ (respectively $P(Y(x,k)=0)$) is the probability that a randomly chosen peptide will be a substrate for enzyme $k$ (respectively, not a substrate).

If we choose $P(Y(x,k)=1) = P(Y(x,k)=0) = 1/2$, then these terms cancel, to obtain
\begin{equation}
  \label{eq:prob_Y}
  P\left(Y(x,k) = 1 | \theta^{(k)}\right) =
  \frac{\prod_{i} \theta^{(k)}_{1,i}(x_i)}{
  \left[ \prod_{i} \theta^{(k)}_{0,i}(x_i)\right] +
  \left[ \prod_{i} \theta^{(k)}_{1,i}(x_i)\right]}
\end{equation}

\paragraph{Estimation of $\theta$:}
We can estimate the $\theta^{(k)}_{y,i}(j)$ values using Bayesian inference.
We begin with a prior probability distribution on each vector $\theta^{(k)}_{y,i}$, which is $\mathrm{Dirichlet}(\alpha^{(k)}_{y,i}(1),\ldots,\alpha^{(k)}_{y,i}(6))$,
where $\alpha^{(k)}_{y,i} = (\alpha^{(k)}_{y,i}(1),\ldots,\alpha^{(k)}_{y,i}(6))$ parameterizes the prior distribution.
A good initial choice for this parameter vector seems to be to choose
$\alpha^{(k)}_{y,i}(j)$ to be constant across $j$, $k$ and $y$, and to only depend upon $i$.
We choose this value to be $1$ when $i=\pm1$, and to increase as $i$ moves further from $0$, which says that amino acids further from the serine are less likely to have a strong influence on activity.

With this prior distribution, our posterior distribution on $\theta^{(k)}_{y,i}$ is also a Dirichlet distribution, but with different parameters.
In particular, it is
$\mathrm{Dirichlet}(
\alpha^{(k)}_{y,i}(1) + N^{(k)}_{y,i}(1),
,\ldots,
\alpha^{(k)}_{y,i}(6) + N^{(k)}_{y,i}(6))$,
where $N^{(k)}_{y,i}(j)$ counts how many peptides $x$ in the training data with $Y(x,k)=y$ had $x_i=j$.  That is, it counts how many peptides had amino acid $i$ in Dayhoff class $j$.
The posterior remains independent across $y$,$i$,$k$.
The expectation of $\theta^{(k)}_{y,i}(j)$ under this posterior distribution is
\begin{equation}
  \label{eq:theta_estimate}
  \theta^{(k)}_{y,i}(j) \approx
  \frac{\alpha^{(k)}_{y,i}(j) + N^{(k)}_{y,i}(j)}{\sum_{j'} \alpha^{(k)}_{y,i}(j') + N^{(k)}_{y,i}(j')}.
\end{equation}

\paragraph{Computation of $P(Y(x,k)=1 | \data)$}
To calculate the probability that $Y(x,k)=1$ given the training data, we must integrate \eqref{eq:prob_Y} over the full posterior distribution $\theta^{(k)}_{y,i}$.
A simpler approximate method for calculating $P(Y(x,k)=1 | \data)$ is to plug in the estimates of $\theta^{(k)}_{y,i}$ from \eqref{eq:theta_estimate} into \eqref{eq:prob_Y}.

\section{Method for Suggesting Which Peptide to Test Next}
\label{sec:design}
One approach to recommending peptides to test, based on estimates obtained from
a statistical model, would be to assume that the estimates of substrate / not
substrate reported by the statistical model were correct, and then find the
shortest peptide that the statistical model estimated to be a substrate for
both enzymes.  However, this approach ignores the fact that predictions from a
statistical model are only correct with some probability.

The following method is more sophisticated, and incorporates the probability of
correctness into what it recommends testing.  If the probabilities of substrate
/ not substrate are all 0 or 1, then we recover the less sophisticated approach.
We use the probabilities of substrate / not substrate as computed by the
statistical method described in Section~\ref{sec:classifier}, but the same
method could be equally well applied with probabilities estimated in another
way.

We first describe a method for computing the value $V(x)$ of synthesizing and testing any given peptide $x$.  Then, the method for recommending which peptide to test next is then to recommend the peptide for which $V(x)$ is the largest.

The method for computing $V(x)$ depends on $L^*$, which is the length of the smallest peptide found so far that is an enzyme for both substrates.
Let $L(x)$ be the length of peptide $x$.
Let
\begin{equation*}
  p(x) = P\left(Y(x,1)=Y(x,2)=1 | \data\right).
\end{equation*}
Under the model described in Section~\ref{sec:classifier}, with $Y(x,1)$ independent of $Y(x,2)$, this is computed as $p(x) = P\left(Y(x,1)=1 | \data\right) P\left(Y(x,2)=1 | \data\right)$, where each $P(Y(x,k)=1 | \data)$ can be computed using either the exact or the approximate method described at the end of Section~\ref{sec:classifier}.

If we test peptide $x$, and $Y(x,1)=Y(x,2)=1$, then the new value of $L^*$ will be $\min(L^*,L(x))$, and the reduction in $L^*$ will be $\max(0,L^*-L(x))$.
If, instead, at least one of $Y(x,1)$ or $Y(x,2)$ is different from $1$, then $L^*$ remains unchanged, and the reduction in $L^*$ is $0$.
We let $V(x)$ be the expected value of this reduction in length, due to testing peptide $x$,
\begin{equation*}
  V(x) = p(x)\max(0,L^*-L(x)).
\end{equation*}

Then, the peptide that we suggest testing next is
\begin{equation*}
  \arg\max_x V(x)
\end{equation*}

This is similar to expected improvement methods, which are used in the optimization literature \cite{JoScWe98},
and to knowledge-gradient method \cite{PoRy12,PoFr08}).

\zap{
\section{Variants on the Classification Method}

\begin{itemize}
\item Rather than building two separate classifiers, one for $Y(x,1)$ and the other $Y(x,2)$, we could build a single classifier for classifying
\end{itemize}

Let the next peptide that we test have left and right endpoints $X_1,X_2$, so its length is $X_2 - X_1 + 1$.  Suppose that $X_2-X_1 + 1 < f^*_n$.  If it turns out that $Y(X_1,X_2)=1$, so this peptide supports catalysis, then the shortest catalyzing peptide we will have found after the test will have length $X_2-X_1 + 1$.  If it turns out that $Y(X_1,X_2)=0$, then the shortest catalyzing peptide after the test will still have length $f^*_n$.  Let $f^*_{n+1}$ be this length of the shortest catalyzing peptide after the additional tests, so the expected value of this shortest length is
    \begin{align*}
      E_n[ f^*_{n+1} ] &= P_n(Y(X_1,X_2)=1) (X_2 - X_1 + 1) + P_n(Y(X_1,X_2)=0) f^*_n \\
      		       &= f^*_n - P_n(Y(X_1,X_2)=1) \left[ f^*_n - (X_2 - X_1 + 1)\right]
    \end{align*}
    where the subscript $n$ in $P_n$ and $E_n$ indicates that probabilities and expectations are taken with respect to the posterior distribution given everything we know after $n$ steps have been performed.

The expected improvement method would then search computationally over all $X_1,X_2$ to find the one that causes this expected shortest peptide length, $E_n[ f^*_{n+1} ]$, to be as small as possible.

In this search, there should be an interesting tradeoff between testing peptides likely to support catalysis against testing those peptides that are short.  This is  because if $X_2 - X_1 + 1$ is close to the length $f^*_n$, then we can have $Y(X_1,X_2)=1$ occur with probability close to $1$, but we save less on the length if it happens.  Alternatively, if $X_2-X_1 + 1$ is much shorter than $f^*_n$, then we would save a lot on length if $X_1,X_2$ turned out to be a catalyzing sub-peptide, but this would be less likely to occur.
}


\paragraph{Computation of expected improvement of a set of peptides}
Given a set of $N$ peptides: $S=(X_n)_{n=1}^N$, we want to compute a value of this set. Suppose the shortest peptide that is substrate for both enzymes in $S$ is $x'$, and $L(x')$ is the length of $x'$, then the value of $S$ can be defined as:

\begin{equation*}
V(S) = \max(0, L^*-L_{x'}).
\end{equation*}

For simplicity, we assume the peptides in $S$ are sorted in the order of increasing length. $\forall n\in\{1,...,N\}$, let $p_{min}(X_n)$ denote the probability that $X_n$ is the shortest peptide that is substrate for both enzymes given training data. Let

\begin{equation*}
Y(x) = Y(x,1)\cdot Y(x,2)
\end{equation*}

Then $Y(x) = 1$ if and only if $x$ is substrate for both enzymes, and

\begin{equation*}
p_{min}(X_n) = P(Y(X_n)=1, Y(X_{n+1})=...=Y(X_N)=0|\data)
\end{equation*}

And the value of $S$ can be represented as:

\begin{equation*}
V(S) = \sum_{x'\in S} p_{min}(x')\max(0,L^*-L(x'))
\end{equation*}

We call $V(S)$ the expected improvement of $S$.

Since $Y(x,k)=1|\data$ and $Y(x',k)=1|\data$ are not independent, we don't have a closed form of $P_{min}(x)$. Therefore we compute $V(S)$ through simulation.

We carry out $M$ iterations of simulation. In each run, we sample $\theta$ from posterior distribution given \data. We then use the sampled $\theta$ to predict $P(Y(X_n,1)=1,Y(X_n,2))=1|\data)$ and sample $Y(X_n,1)$ and $Y(X_n,2)$ from the prediction for all $n\in\{1,...,n\}$. Then we choose the peptide $x$ with shortest length and $Y(x,1)=Y(x,2)=1$, and calculate the reduction in length compared with $L^*$. Finally we average the reduction calculated in each iteration to get expected improvement.

\begin{algorithmic}[1]
\REQUIRE inputs: $M$, \data, $S=(X_n)_{n=1}^N$ and $L^*$
\STATE $(P_n)_{n=1}^N \gets 0$
\STATE $(I_n)_{n=1}^N \gets 0$
\FOR { $n=1$ to $N$ }
\STATE $R_n \gets \max(0,L^*-L(X_n))$
\ENDFOR
\STATE $(\text{maxR}_i)_{i=1}^M \gets 0$
\FOR{$i=1$ to $M$}
\STATE $\theta \gets \text{sample from posterior distribution given \data}$
\STATE $(P_n)_{n=1}^N \gets \text{predict from Naive Bayes classifier }\theta$
\FOR{ $n=1$ to $N$ }
\STATE $I_n \gets \text{sample from } \operatorname{Bernoulli}(P_n)$
\ENDFOR
\STATE $\text{maxR}_i \gets \max\limits_{1\leq n\leq N}I_n\cdot R_n$
\ENDFOR
\STATE $V(S)\gets \frac{1}{N}\sum_{i=1}^N\text{maxR}_i$
\end{algorithmic}

\paragraph{Knowledge Gradient method}
We introduce a knowledge-gradient approach to find a set of peptides with a large expected improvement. The problem is as follows: given a set of $N$ randomly chosen a peptides $S=(X_n)_{n=1}^N$, we want to modify some peptides in $S$ so that the expected improvement $V(S)$ can be as large as possible. To achieve this goal, we iteratively carry out the following process: we first randomly choose a peptide in $S$ and a position in that peptide, and we use a knowledge gradient policy to decide which peptide class in that position will maximize the expected improvement of $S$. This policy works as follows.

Let $C$ be the number of classes in our reduced alphabet of amino acids. We have $C$ alternative classes for the selected position in the selected peptide. Let $\mathcal{C}=\{1,...,C\}$ be our set of choices and $c\in\mathcal{C}$ represent an alternative. Let $V_c(S)$ denote the expected improvement given $c$ is the amino acid class at the chosen position. We assume $V_c(S)\sim\mathcal{N}(\mu_c,\sigma_c^2)$.

Given our measuring budget $K$(the maximum number of measurements), in each measurement, we choose an alternative $c\in\{1,...,C\}$ and simulate the desired expected improvement once and update our belief about $V_c(S)$. After $K$ measurements, we simply pick the class with highest $\mu$ to be the class at the chosen position. The key problem is which alternative we choose to measure at each iteration.

After $k$ iterations, we let $\mu^k$ be our vector of means and $\beta^k$ vector of precisions(inverse of variances). We compute a knowledge gradient factor $\nu^{KG,k}_c$ of each alternative $c$. Let $c^{k+1}$ denote the alternative we choose to measure in the $(k+1)$th iteration. The knowledge gradient policy assigns

\begin{equation*}
c^{k+1} = \arg\max\limits_{c\in\mathcal{C}}\nu^{KG,k}_c
\end{equation*}

After simulating the expected improvement of $S$: $W^{k+1}$, assuming $c^{k+1}$ is the class at the chosen position, we update our belief about $V(S)$ using the following equations:

\begin{eqnarray*}
\mu_c^{k+1} &=& \left\{\begin{array}{ll}
\frac{\beta_c^k\mu_c^k + \beta_c^W W^{k+1}}{\beta_c^k+\beta_c^W} & \text{if } c^{k+1}=c \\
\mu_c^k & \text{otherwise}\end{array}\right. \\
\beta_c^{k+1} &=& \left\{\begin{array}{ll} 
\beta_c^k + \beta_c^W & \text{if } c^{k+1} = c\\
\beta_c^k & \text{otherwise}\end{array}\right.
\end{eqnarray*}

Here $\beta^W$ is the measurement precision. To obtain $\beta^W_c$ for alternative $c$, we simply simulate the corresponding expected improvement for a small number of times and use the inverse of sample variance of our simulated data as the measurement error. 

Below is the process of computing the knowledge gradient factor $\nu^{KG,k}$. First, we compute the the variance in our belief of $\mu^{k+1}$ given $\mu_k$:

\begin{eqnarray*}
\tilde{\sigma}_c^{2,k} & = & Var[\mu^{k+1}_c|\mu^k,\beta^k] \\
                       & = & Var[\mu^{k+1}_c - \mu_c^k|\mu^k,\beta^k]
\end{eqnarray*}

It can be shown that

\begin{eqnarray*}
\tilde{\sigma}_c^{2,k} & = & \sigma^{2,k}_c-\sigma^{2,k+1}_c \\
                       & = & (\beta_c^k)^{-1}-(\beta_c^k+\beta_c^W)^{-1}
\end{eqnarray*}.

The knowledge gradient is found by first computing

\begin{equation*}
\zeta^k_c = -\left|\frac{\mu_c^k-\max_{c'\ne c} \mu_{c'}^k}{\tilde{\sigma}_c^k}\right|
\end{equation*}

which is called the normalized influence of decision $c$. We then compute 

\begin{equation*}
f(\zeta) = \zeta\Phi(\zeta)-\phi(\zeta)
\end{equation*}

where $\Phi(\zeta)$ and $\phi(\zeta)$ are cumulative standard normal distribution and normal density respectively. Finally, the knowledge gradient factor is computed as

\begin{equation*}
\nu^{KG,k}_c = \tilde{\sigma}_c^{k}f(\zeta_c^k)
\end{equation*}


\bibliography{catalysis}
\bibliographystyle{apalike}




\end{document}

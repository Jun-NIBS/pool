\documentclass[11pt]{article}
\usepackage[latin1]{inputenc}
\usepackage[T1]{fontenc}


\title{The Optimality of the Greedy Algorithm for\\ the Batch Selection Problem}
\date{\today}
\author{MP}

%\author{Matthias Poloczek\thanks{School of Operations Research and Information Engineering, Cornell University, Ithaca, NY, USA. Email:~\texttt{poloczek@cornell.edu}. Supported by the Alexander von Humboldt Foundation within the Feodor Lynen program and by NSF grant CCF-1115256.} 
%}
%\institute{School of Operations Research and Information Engineering, Cornell University \email{poloczek@cornell.edu}}


\usepackage{lmodern}				% nice font for PDF -- preferable over ae package
\usepackage{fullpage}
\usepackage{microtype}
\usepackage{hyperref}				% clickable links
\usepackage{graphicx}
\usepackage{xspace}

%% AMS Mathe Packet laden - der Nachfolger von amstex
\usepackage[cmex10]{amsmath}
\usepackage{amssymb,amsthm,url}
\usepackage[small]{complexity}			% for complexity classes

\newtheorem{theorem}{Theorem}
%ftp://ftp.ams.org/pub/tex/doc/amscls/amsthdoc.pdf
\newtheorem{definition}{Definition}
\newtheorem{corollary}{Corollary}
\newtheorem{lemma}{Lemma}
\newtheorem{fact}{Fact}
\newtheorem{proposition}{Proposition}
\newtheorem{conjecture}{Conjecture}
\newtheorem{remark}{Remark}
\newtheorem{claim}{Claim}
\newtheorem{observation}{Observation}

\newcommand\maxkcover{\textsc{Max~$k$-Cover}\xspace}

\renewcommand{\E}{\ensuremath{\mathbb{E}}}		% symbol for expected value

\newcommand{\commentMP}[1]{\marginpar{\tiny\color{blue} MP: ``#1''}}
\newcommand{\commentPF}[1]{\marginpar{\tiny\color{black} PF: ``#1''}}
\newcommand{\commentJW}[1]{\marginpar{\tiny\color{red} JW: ``#1''}}



%TODO Remove before submitting!
\overfullrule=5pt				% zeige ï¿½bervolle Boxen an, S. 985
\allowdisplaybreaks				% erlaubt das Umbrechen von Seiten innerhalb von Formeln
\pagestyle{plain}


\begin{document}
\maketitle

We formalize the \emph{Batch Selection Problem} as follows. The input consists of a set of peptides~$P = \{P_1, P_2,\ldots,P_m\}$ and a threshold value~$b \in \mathbb{N}$. Moreover, there is an oracle~$\cal O$ that given any collection of peptides~$S \subseteq P$ returns the probability that~$S$ contains a ``hit'', i.e.\ that at least one of the peptides in~$S$ possesses certain pre-specified properties.
%
The goal is to find a selection of~$b$ peptides, called ``batch'', that maximizes this probability. Note that we can always assume that the optimal batch contains exactly~$b$ peptides, since adding more peptides to the batch never decreases the probability that it contains a hit.

We define the function~$f: {\cal P}(P) \to [0,1]$ as~$f(S) = \mathrm{prob}[\text{$S$ contains a hit}]$ for all~$S \in {\cal P}(P)$ and let~$$\mathrm{Opt} = \sup_{S \in {\cal P}(P), |S| = b} f(S),$$ where~${\cal P}(P)$ denotes the power set of~$P$. Note that~$f$ is normalized ($f(\emptyset) = 0$), monotone, and submodular. 
%
Furthermore, we point out that the Batch Selection Problem is equivalent to maximizing~$f$ over a uniform matroid.
%
Therefore the greedy algorithm of Nemhauser, Wolsey, and Fisher~\cite{nwf78} finds a set of~$b$ peptides that contains a hit with probability at least~$(1 - 1/e) \cdot \mathrm{Opt}$.

In this section we study whether this worst case guarantee can be improved. The answer turns out to be ``no'', as we will show that no polynomial time algorithm can achieve an approximation ratio better than~$1 - 1/e$ under the standard hardness assumption $\P \neq \NP$.
%
Therefore, somewhat surprisingly, the very simple greedy algorithm has an optimal approximation guarantee among efficient algorithms.\footnote{An algorithm is called \emph{efficient} if its running time is bounded by a polynomial in the input size. Moreover, an efficient algorithm is an~$\alpha$-approximation algorithm (or has approximation ratio~$\alpha$) for a maximization problem if for every input it returns a solution of value at least~$\alpha$ times the optimum.}
%
\begin{theorem}
\label{theorem_hardness_batch_selection}
No efficient algorithm can approximate the Batch Selection Problem within~$1 - 1/e + \varepsilon$ for any~$\varepsilon > 0$ (unless~$\P = \NP$).
\end{theorem}
\begin{proof}
%We give an approximation-preserving reduction from the \maxkcover problem. Then Theorem~\ref{theorem_hardness_batch_selection} follows from a result of Feige (cp.\ Theorem~5.3 in~\cite{})  that \maxkcover cannot be approximated better than~$1 - 1/e$ by any polynomial time algorithm (assuming $\P \neq \NP$).
We will show that an efficient algorithm that approximates the Batch selection problem within~$1 - 1/e + \varepsilon$ can be used to approximate the \maxkcover problem with the same guarantee in polynomial time. But this contradicts a result regarding the approximability of \maxkcover given by Feige (cp.\ Theorem~5.3 in~\cite{feige98}) stating that no efficient algorithm can approximate \maxkcover within~$1 - 1/e + \varepsilon$ (unless $\P = \NP$).

Let us begin with stating the \maxkcover problem formally: We are given a natural number~$k$, a universe~${\cal U} = \{e_1,\ldots,e_n\}$ of~$n$ disjoint elements, and~$m$ sets~$S_1,\ldots,S_m \in {\cal P}(U)$. Our goal is to select~$k$ sets such that their union contains as many elements of~$\cal U$ as possible.

Given any instance~$I \in$ \maxkcover we transform~$I$ to an instance~$I'$ of the Batch Selection Problem as follows: First we create a new (and unique) peptide~$P_i$ for each set~$S_i$ for all~$1 \leq i \leq m$. Moreover, we set the threshold value~$b := k$.
%
The implementation of~$\cal O$ is the critical step, since we must ensure that the transformation runs in polynomial time, measured in the input size of the \maxkcover instance. Thus, we cannot afford to write down~$\cal O$ explicitly as the table of function~$f$. We use the flexibility we have in choosing the probabilities of the instance~$I'$ of the Batch Selection problem in order to give an efficient implementation of the oracle, assuming access to the \maxkcover instance~$I$:
Specifically, we set the probability that a set~$S \in {\cal P}(P)$ contains a hit to~$\frac{1}{|\cal U|}$ times the number of elements in~$\bigcup_{P' \in S} P'$. Note that all probabilities are at least zero and at most one, and hence well-defined.
%
In particular, the probability for any~$S \in {\cal P}(P)$ can be computed in time~$O(|{\cal U}| + \sum_{P' \in S}{|P'|}) = O(m\cdot n)$ using a Boolean array of size~$|{\cal U}| = n$: we iterate over the sets corresponding to the peptides in~$S$ and mark the elements of~$U$ we encounter in the array.

The central observation is that if we are given a solution to~$I'$, i.e.\ a set of peptides that contains a hit with probability~$\nu$, we can reconstruct a selection of~$k$ sets that cover~$\nu \cdot |{\cal U}|$ elements for~$I$, using the bijection between sets in~$I$ and peptides in~$I'$.
%
Moreover, the optimal solution to the~\maxkcover instance covers~$c$ elements if and only if the best batch contains a hit with probability~$\frac{c}{|\cal U|}$ for this choice of probabilities. 

Thus, if there is an efficient algorithm that achieves an approximation ratio better than~$1 - 1/e + \varepsilon$ for the Batch selection problem (for any~$\varepsilon > 0$), then combining it with our transformation yields an algorithm that approximates~\maxkcover within~$1 - 1/e + \varepsilon$.
%
Moreover, this algorithm runs in time~$poly(n + m)$ due to the closure of polynomials under multiplication.
%
Thus, we have obtained a contradiction to the result of Feige and our claim follows.
\end{proof}
%
\begin{remark}
Note that our formalization of the Batch Selection problem differs from the problem at hand in two aspects:

First, in practice we are only interested in peptides of length~$c$ for a small constant~$c < 10$. Since there are only twenty amino acids, and moreover these are bundled into eight groups, the number of different peptides is a constant itself, although a very large one. Hence the problem we face in practice is not~$\NP$-hard in the   technical sense.

Second, the probabilities that peptides are hits under the Bayesian model have a very specific structure that is abstracted in our formalization. In particular, the probabilities are determined by the data \emph{and} the statistical model. 
%
Thus, the inapproximability result given in Theorem~\ref{theorem_hardness_batch_selection} points us to the question whether an algorithm can be tailored to exploit this structure in order to achieve a better approximation.
\end{remark}

\bibliographystyle{abbrv}
%%\bibliographystyle{alpha}			% Stil der Verweise
\bibliography{peptides}				% Verweis auf .bib Datei


\end{document}


